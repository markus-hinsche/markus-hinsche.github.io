<!DOCTYPE html>
<html>
  <head>
    <title>Productive machine learning in Python with Palladium and scikit-learn</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }

    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Productive machine learning in Python

Markus Hinsche, Senior ML Engineer, Freelance, datascienceretreat.com 2020

---

# Agenda

We'll talk about:

- A seamless workflow for developing machine learning models and
  putting them in production

- Solving common problems in machine learning projects, such as
  modularization of code, managing configuration, hyperparameter
  optimization, and distributed learning, and working with big data

- Making available trained models through scalable and secure web
  services

---

# Download slides and code

Download this repository to be able to run code examples:

```bash
$ git clone https://github.com/markus-hinsche/tut-productive-ml.git
```

Slides are online at:

https://markus-hinsche.github.io/tut-productive-ml/

---

# whoami

- Machine Learning Freelancer ([website](http://markushinsche.de), [github](https://github.com/markus-hinsche/))
  * Hands-on Implementation and Consoluting (Machine Learning)
- Projects: Welthungerhilfe (computer vision), The Martec (NLP), ...
- Strengths:
  * Python (since 2009)
  * Deep Learning (since 2018)
  * Software development best practices (testing, pairing, clean code)
  * Mentoring

---

class: center, middle

# Why ML in production?

### "87% of data science projects never make it to production"

[https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/]

### "Model deployment is more and more expected from a Data Scientist"

[Jose]

???

Maybe on demo day, you want to deploy some project.
Maybe you can learn how to do this.

---

# What is the difference?

* code for researching models
* code for deploying models into production

???

clean code: consistent style, tested, peer reviewed

Collaboration
- git/github in real life
- work in codebase with other people

environments: dev, staging, prod

---

# Palladium: Overview

Palladium emerged from a Otto Group BI project to predict parcel
delivery times.

A presentation of Palladium from PyData 2016 is found [here
(PDF)](./resources/palladium-pydata-lattner.pdf).

## Key features

- Establish common workflow across teams and roles
- Avoid boilerplate in ML projects
- Smooth transition from prototypes to production
- Scalability
- Avoid license costs

---

# Palladium: Different roles

**Individual Data Scientists**:

- prototype quickly and avoid boilerplate
- organize code and configuration
- establish an interface with engineers, deliver API

**Data Science Teams**:

- use a common workflow to access dataset, to model, and evaluate
- try different algorithms/models on the same problems without hassle
- try new data with the same algorithm, and compare
- combine flexible configuration with version control to ensure
  reproducibility

**Production Engineers**:

- can deploy a Data Scientist's model easily
- can work on production aspects along well-defined lines, without
  interfering with work of Data Scientist

---

# Palladium: Similar projects

- [Facebook Hydra](https://cli.dev/): *A framework for elegantly
  configuring complex applications*

- [MLflow](https://www.mlflow.org/): *An open source platform
  for managing the end-to-end machine learning lifecycle*

---

# Iris dataset

What's it like to work with Palladium?  Let's get a sense of that by
implementing a simple classifier.

In our first example in `step1`, we will make use of the classic
"Iris" machine learning dataset from 1936.

`step1/iris.data` contains the data in CSV format.  Note that the
first line for column names is missing:

```csv
5.2,3.5,1.5,0.2,Iris-setosa
4.3,3.0,1.1,0.1,Iris-setosa
5.6,3.0,4.5,1.5,Iris-versicolor
6.3,3.3,6.0,2.5,Iris-virginica
...
```

The columns are:

- `sepal length`
- `sepal width`
- `petal length`
- `petal width`
- `species`

---

# Iris as a classification problem

We want to implement a classifier that predicts the species based on
the four attributes.

![:scale 50%](./resources/iris-scatter.png)

---

# Palladium: Reading the dataset

You'll find a configuration file in `step1/palladium-config.py`.
Therein, you'll find the definition of two `dataset_loader`s, which
are responsible for reading the dataset:

```python
{
    'dataset_loader_train': {
        '__factory__': 'palladium.dataset.CSV',
        'path': 'iris.data',
        'names': [
            'sepal length',
            'sepal width',
            'petal length',
            'petal width',
            'species',
        ],
        'target_column': 'species',
        'nrows': 100,
    },

    'dataset_loader_test': {
        '__copy__': 'dataset_loader_train',
        'nrows': None,
        'skiprows': 100,
    },
}
```

---

# Palladium: Configuration and code separated

Palladium encourages the separation of configuration and code.
Machine Learning projects typically have a lot of constants and
configuration.  Here are some examples:

- Path to the dataset

```python
        'path': 'iris.data',
```

- Database configuration

```python
        'url': 'sqlite:///iris-model.db',
```

- Hyperparameters of our models

```python
        'C': 0.1,
```

Also, we frequently have to deal with differing configuration between
production and development environments.

---

# Palladium: Model definition

```python
    'model': {
        '__factory__': 'sklearn.linear_model.LogisticRegression',
        'C': 0.1,
    },

    'model_persister': {
        '__factory__': 'palladium.persistence.Database',
        'url': 'sqlite:///iris-model.db',
    },
```

The model definition uses the configuration key `model`.  As our
classifier, we'll make use of the implementation of [Logistic
Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
out of scikit-learn.

The `model_persister` entry defines where we'll save the trained model
and where to read it back from.  The configuration above configures a
SQLite database to act as the model storage.

---

# Palladium: Installation

Let's install Palladium.  We'll first create a
[virtualenv](https://virtualenv.pypa.io), and install dependencies
therein:

```bash
$ python3 -m venv .
$ source bin/activate  # or: `scripts/activate.bat` on Windows
$ pip install -U pip
$ pip install -r step1/requirements.txt
Collecting palladium (from -r step1/requirements.txt (line 1))...
...
Successfully installed ...
```

---

# Palladium: Installation - possible issues

Maybe you're on Windows and you have trouble installing
[ujson](https://pypi.org/project/ujson/) because you don't have Visual
Code installed.  Then check out this site which has [binary wheels for
ujson for Windows](https://www.lfd.uci.edu/~gohlke/pythonlibs/#ujson).
Here's an example of how you would install ujson from that site (for
Python 3.6):

```bash
$ pip install https://download.lfd.uci.edu/pythonlibs/r5uhg2lo/ujson-1.35-cp36-cp36m-win_amd64.whl
```

If you really want to use [Conda](https://conda.io) instead of vanilla
Python:

```bash
$ conda install ujson
$ pip install -r step1/requirements.txt
```

> ⚠️ **Note:** For this tutorial, it's not recommended to use Conda.
Use it if you know what you're doing, but be aware that some things
will work differently for you going forward.

---

# Palladium: Training a model

We already have all pieces in place to fit our first model.  To fit,
we'll use Palladium's `pld-fit` command along with the `--evaluate`
option.  On Linux:

```bash
$ cd step1
$ pld-fit --evaluate
INFO:palladium:Loading data...
...
INFO:palladium:Train score: 0.9
...
INFO:palladium:Test score:  0.82
...
INFO:palladium:Wrote model with version 1.
```

Our classifier reaches an accuracy of 90% on the training set and 82%
on the test set.

---

# Regularization

So what is this `C` parameter of our model about?

![:scale 80%](./resources/ng-regularized-lr.png)

---

# Regularization (2)

From the [scikit-learn
docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html):

> `C`: Inverse of regularization strength; must be a positive
float. Like in support vector machines, smaller values specify
stronger regularization.

## ✍️ Exercise

Can we find a better C parameter such that our score on the test set
improves?

---

# Grid search

The search for optimal hyperparameters is a very common task in
machine learning.  In our example, doing the search by hand was still
easy, but more complex models and pipelines often have a big number of
these parameters.

There's different options to perform the search, scikit-learn itself
implements a [number of these
algorithms](http://scikit-learn.org/stable/modules/grid_search.html).
One of these algorithms is called *grid search*.

> ⚠️ **Note:** The [scikit-optimize
package](https://scikit-optimize.github.io/) allows us to search for
hyperparameters using *Bayesian optimization*.  The Palladium docs
contain [an example of using
`skopt.BayesSearchCV`](https://github.com/ottogroup/palladium/blob/master/docs/user/faq.rst#can-i-use-bayesian-optimization-instead-of-grid-search-to-tune-my-hyperparameters)
instead of `GridSearchCV`, which you're encouraged to try out.

---

# Grid search: Application

`step2/palladium-config.py` contains the parametrization of our grid
search:

```python
    'grid_search': {
        'param_grid': {
            'C': [0.1, 1, 10, 100, 1000],
         },
        'cv': 8,
        'verbose': 4,
        'n_jobs': -1,
    },
```

We use Palladium's `pld-grid-search` command to execute the search:

```bash
$ cd ../step2
$ pld-grid-search
```

---

# Grid search: Interpreting the output

```
   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C
3       0.000778         0.000188             0.98          0.989990     100
4       0.000860         0.000221             0.98          1.000000    1000
2       0.000691         0.000198             0.96          0.972828      10
1       0.000675         0.000199             0.95          0.955667       1
0       0.001668         0.000427             0.87          0.872882     0.1
```

A low value for C means stronger regularization.  The effect here is
"underfitting", which is characterized by a low train and low test
score.

![:scale 100%](./resources/sklearn-overfitting-underfitting.png)

---

# Grid search and cross validation

We set the `cv` parameter in our grid search configuration to 8, which
means we want to use 8-fold cross validation.  Let's take a look again
at the output of `pld-grid-search`:

```
...
Fitting 8 folds for each of 5 candidates, totalling 40 fits
...
        params  split0_test_score  split1_test_score  split2_test_score  \
3   {'C': 100}           1.000000           1.000000           1.000000

   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \
3           0.916667           0.916667           1.000000           1.000000

   split7_test_score  mean_test_score  std_test_score  rank_test_score  \
3           1.000000             0.98        0.035590                1
...
```

Using cross validation is very useful if you have a small dataset but
you still want to get a good idea of how our model's real life
performance will look like.

---

# Palladium: Web service

Palladium implements a web service, with which we can use our trained
model via HTTP to make predictions.

First we need to configure the web service.  In our Iris example, this
is what we need:

```python
    'predict_service': {
        '__factory__': 'palladium.server.PredictService',
        'mapping': [
            ('sepal length', 'float'),
            ('sepal width', 'float'),
            ('petal length', 'float'),
            ('petal width', 'float'),
        ],
    },
```

You can find this configuration in `step3/palladium-config.py`.

---

# Palladium: Running the web service

```bash
$ cd ../step3
$ pld-fit --evaluate
$ pld-devserver
```

We can now call the service via HTTP GET.  Follow this link:

http://localhost:5000/predict?sepal%20length=6.3&sepal%20width=2.5&petal%20length=4.9&petal%20width=1.5

This is the result:

```json
{
    "metadata": {"error_code":0,"status":"OK"},
    "result":   "Iris-virginica"
}
```

---

# Palladium: Python API

It's easy using Palladium's configuration and modules outside of the
`pld-*` scripts and the built-in web service.  For example, you may
want to implement your own web service, use Palladium from within your
Jupyter Notebook, or write a custom command-line script that uses it.

Let's take a look at how this works.  We want to write a script that
uses our Iris model to predict and produce the following output:

```bash
$ python predict.py 6.3 2.5 4.9 1.5
Iris-setosa: 0.0%
Iris-versicolor: 44.9%
Iris-virginica: 55.1%
```

---

# Palladium: Python API (2)

Here's the implementation of the script, which you also find in
`step3/predict.py`:

```python
import sys
from palladium.config import get_config

def predict(features):
    # Get hold of the Palladium configuration in palladium-config.py:
    config = get_config()
    # Use the model_persister to load the trained model:
    model = config['model_persister'].read()
    # From here on, it's plain scikit-learn:
    result = model.predict_proba([features])[0]
    for class_, proba in zip(model.classes_, result):
        print("{}: {:.1f}%".format(class_, proba*100))

if __name__ == '__main__':
    predict([float(v) for v in sys.argv[1:]])
```

---

# Skorch: Installation

[Skorch](https://skorch.readthedocs.io/) is a scikit-learn compatible
library for neural networks.  It's based on [PyTorch](https://pytorch.org).

First install skorch and dstoolbox:

```bash
$ pip install -r step4/requirements-base.txt  # install skorch and dstoolbox
```

Then, use the [installation matrix on the PyTorch
website](https://pytorch.org/get-started/locally/) to find out how to
install PyTorch on your computer:

![:scale 80%](./resources/pytorch-start-locally.png)

---

# Why PyTorch?

Here's from Stefano J. Attardi's blog post titled [How I Shipped a
Neural Network on iOS with CoreML, PyTorch, and React
Native](https://attardi.org/pytorch-and-coreml):

> Like most people, I cut my neural teeth on TensorFlow. But my
honeymoon period had ended. I was getting weary of the kitchen-sink
approach to library management, the huge binaries, and the extremely
slow startup times when training. TensorFlow APIs are a sprawling
mess. Keras mitigates that problem somewhat, but it’s a leaky
abstraction. Debugging is hard if you don’t understand how things work
underneath.

> PyTorch is a breath of fresh air. It’s faster to start up, which
makes iterating more immediate and fun. It has a smaller API, and a
simpler execution model. Unlike TensorFlow, it does not make you build
a computation graph in advance, without any insight or control of how
it gets executed. It feels much more like regular programming, it
makes things easier to debug, and also enables more dynamic
architectures – which I haven’t used yet, but a boy can dream.

---

# Skorch: Iris

We'll continue working with the Iris dataset.  Instead of a logistic
regression, we'll use a neural network to do the classification.

You'll find the implementation of the neural network and supporting
code in `step4/model.py`.  The configuration is in
`step4/palladium-config.py`.  Let's take a look at the most important
parts.

A large part of our configuration looks exactly the same as in the
last step, namely `dataset_loader_train`, `dataset_loader_test` and
`model_persister`.

The `model` entry is where things get more interesting:

```python
    'model': {
        '__factory__': 'model.create_pipeline',
    },
```

Our model is produced by a custom function called `create_pipeline`.

---

# scikit-learn: Pipelines

A `sklearn.pipeline.Pipeline` is a pipeline of transforms with a final
estimator (classifier or regressor).  Here's the implementation of
`model.create_pipeline`:

```python
def create_pipeline(
    device='cpu',  # or 'cuda'
    max_epochs=50,
    lr=0.1,
    **kwargs
):
    return PipelineY([
        ('cast', Cast(np.float32)),
        ('scale', StandardScaler()),
        ('net', NeuralNetClassifier(
            MyModule,
            device=device,
            max_epochs=max_epochs,
            lr=lr,
            train_split=None,
            **kwargs,
        ))],
        y_transformer=LabelEncoder(),
        predict_use_inverse=True,
        )
```

---

# scikit-learn: StandardScaler

`StandardScaler` is one of the transforms that scikit-learn itself
implements.  This transform standardizes features by removing the mean
and scaling features to unit variance.

From the [scikit-learn
docs](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html):

> Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the individual
features do not more or less look like standard normally distributed
data (e.g. Gaussian with 0 mean and unit variance).

> For instance many elements used in the objective function of a
learning algorithm (such as the RBF kernel of Support Vector Machines
or the L1 and L2 regularizers of linear models) assume that all
features are centered around 0 and have variance in the same
order. **If a feature has a variance that is orders of magnitude
larger than others, it might dominate the objective function and make
the estimator unable to learn from other features correctly as
expected.**

---

# Skorch: NeuralNetClassifier

```python
from skorch import NeuralNetClassifier

NeuralNetClassifier(
    module=MyModule,
    device='cpu',
    max_epochs=50,
    lr=0.1,
    train_split=None,
    **kwargs,
)
```

Class `NeuralNetClassifier` implements a training loop, recording
metrics, and validation, so you don't have to implement those.

It also implements methods such as `fit` and `predict`, allowing the
use of PyTorch models in the context of scikit-learn.

---

# PyTorch: nn.Module

Our PyTorch `Module` contains the definition of our neural network.
We instantiate the layers of our network in `__init__`.  In method
`forward` we accept the data (`X`) and run the data through our
layers:

```python
class MyModule(nn.Module):
    def __init__(self, num_inputs=4, num_units=10, num_outputs=3):
        super(MyModule, self).__init__()

        self.dense0 = nn.Linear(num_inputs, num_units)
        self.nonlin = F.relu
        self.dropout = nn.Dropout(0.5)
        self.dense1 = nn.Linear(num_units, 10)
        self.output = nn.Linear(10, num_outputs)

    def forward(self, X, **kwargs):
        X = self.nonlin(self.dense0(X))
        X = self.dropout(X)
        X = F.relu(self.dense1(X))
        X = F.softmax(self.output(X), dim=-1)
        return X
```

---

# Learning rate

One of the most important hyperparameters of neural networks is the
*learning rate*.  It defines, how large our iterative steps are when
doing gradient descent.  Gradient descent is the optimization
algorithm that's used in neural networks and many other machine
learning algorithms.

![:scale 80%](./resources/ng-learning-rate.png)

---

# Grid search: Network hyperparameters

In `step4/palladium-config.py`:

```python
    'grid_search': {
        'param_grid': {
            'net__lr': [0.03, 0.1, 0.3],
        },
        'cv': 5,
        'verbose': 4,
        'n_jobs': 1,
    },
```

```bash
$ pld-grid-search
```

## ✍️ Exercise

Which learning rate is the best?

---

# Grid search: Network hyperparameters (2)

## ✍️ Exercise

In `step4/palladium-config.py`, look for the comment *YOUR CODE HERE*.
Try to find a good combination of learning epochs (`net__max_epochs`)
and learning rate (`net__lr`).  Can you find a combination that gets a
`mean_test_score` of 96% or better?

---

# Skorch: EarlyStopping and LRScheduler

- The
  [skorch.callbacks.EarlyStopping](https://skorch.readthedocs.io/en/latest/callbacks.html#skorch.callbacks.EarlyStopping)
  callback allows the stopping of training if further training leads
  to no more improvements.

- The
  [skorch.callbacks.LRScheduler](https://skorch.readthedocs.io/en/latest/callbacks.html#skorch.callbacks.LRScheduler)
  allows us to adapt the learning rate during training, based on
  arbitrary conditions.

- We can use any learning rate scheduler policy implemented in
  [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html),
  such as
  [CyclicLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CyclicLR).

Usage example:

```python
from skorch.callbacks import EarlyStopping, LRScheduler
from torch.optim.lr_scheduler import CyclicLR

NeuralNetClassifier(
    module=MyModule,
    # ...
    callbacks=[
        ('early_stopping', EarlyStopping(patience=10)),
        ('lr_scheduler', LRScheduler(policy=CyclicLR, base_lr=0.001, max_lr=0.1)),
    ],
)
```

---

# Grid search: Parallelization with Dask

If we now consider adding more hyperparameters to the search, such as
the number of neurons in our network's *hidden layer*, we'll
understand that the search can quickly take a long time.

```python
    'grid_search': {
        'param_grid': {  # 3 * 4 * 4 * 5 folds = 240 fits!
            'net__lr': [0.03, 0.1, 0.3],
            'net__max_epochs': [50, 100, 200, 400],
            'net__module__num_units': [5, 10, 20, 40],
        },
```

The search for the best hyperparameters is often the most
computationally intensive part of machine learning projects.  One
convenient way to distribute the hyperparameter search with
scikit-learn among a cluster of computers is by using the
[Dask](http://dask.pydata.org) integration.  From Dask's homepage:

> *Dask provides advanced parallelism for analytics, enabling
performance at scale for the tools you love*

---

# Dask: One scheduler, many workers

```bash
$ pip install -U dask distributed bokeh scikit-learn
```

Open up a new terminal window and **start the Dask scheduler**:

```bash
$ source bin/activate  # on Windows: scripts/activate.bat
$ dask-scheduler
```

Keep the scheduler running.  Now open another terminal window and
**start a Dask worker**.  You may start an arbitrary number of these
workers, on any number of computers:

```bash
$ source bin/activate  # on Windows: scripts/activate.bat
$ cd step4
$ export PYTHONPATH=.  # on Windows: set PYTHONPATH=.
$ dask-worker 127.0.0.1:8786  # IP and port of scheduler
```

The Dask scheduler includes a dashboard, which in the *Workers* tab
lists all the workers that are registered with the scheduler.  The
default address of the dashboard is http://localhost:8787

---

# Dask: Run the grid search

Finally, we can start our grid search in another new terminal as
follows; again, activating the virtual environment, and changing into
the `step4` directory:

```bash
$ source bin/activate  # on Windows: scripts/activate.bat
$ cd step4
$ export PALLADIUM_CONFIG=palladium-config.py,dask-config.py
$ # or on Windows: set PALLADIUM_CONFIG=palladium-config.py,dask-config.py
$ pld-grid-search --save-results=grid-search.csv
```

The 240 fits take around 130 seconds on my laptop (using a single
worker).  How long do they take for you?  (Look for the line `Running
grid search done in...` in the output of `pld-grid-search`.)

Also note that the Dask dashboard includes a *Profile* tab with a
*flame graph* that shows which parts of the Python code you're running
use how much CPU time.  [Check it out.](http://localhost:8787/profile)

---

# Dask: ✍️ Exercise: Set up a cluster

Get together in groups of two or three.  Decide which one of you runs
the central scheduler, start it, and remember its network address.
Make sure you are in the same network (LAN) as the others in your
group.  Now start up a worker on each one of your computers and
connect it to the scheduler.  Use the dashboard to verify that all
Dask workers are indeed connected.

The one in your group running the scheduler can now run the
`pld-grid-search` command as before, and you should see it distribute
the work among all the workers.

> ⚠️ **Note:** Again, make sure that you start the worker like before,
in the right directory, with the environment active, and with
`PYTHONPATH` set.  `pld-grid-search` also needs to be run like before,
with correct `PALLADIUM_CONFIG` and venv.  Lastly, make sure you point
the workers to the right network address using the command-line
argument.

How much faster can you make the grid search go using your cluster?

By the way: The Dask docs have guides on [how to set up Dask
clusters](https://docs.dask.org/en/latest/setup.html) in different
environments, such as with YARN, Kubernetes, and more.

---

# Dask dataframes for analyzing big data

Dask dataframes are useful when the dataset you want to analyze is
larger than your machine's RAM.

```bash
$ pip install dask[dataframe]
```

In our example script in `step4/dd.py`, we create a number of CSV
files with random values (in `create_csvs`).  These are relatively
small files and they'll fit in your RAM (they're about 30MB when
loaded).

We then compare the use of pandas to compute the means of each column
(in `compute_mean_pandas`) with the use of Dask dataframes to do the
same (in `compute_mean_dask`).

You can run the script and compare the times it takes for each method:

```bash
$ python dd.py
```

---

# Dask dataframes for analyzing big data (2)

When you ran the script, Dask defaulted to the [Single-Machine
Scheduler](https://docs.dask.org/en/latest/setup/single-machine.html)
to perform the computation.  That is, it wasn't using the distributed
`dask-scheduler` and workers that we set up previously.

The Single-Machine Scheduler is still useful in that it allows the use
of local threads or processes to split up the work, and it allows us
to work on data that is larger than memory.

## ✍️ Exercise

Go back to your group from the previous exercise and run the `dd.py`
example script such that it uses your group's distributed cluster.

To use your cluster's central scheduler, you must modify the line in
`dd.py` that says "*# use distributed scheduler*."

---

# Determine wine quality with skorch

Let's now go back to the Iris neural net classifier that we developed
previously and apply it to a different dataset.

We'll use the [Wine Quality Data Set](https://www.openml.org/d/40691)
with the goal to model wine quality based on physicochemical tests
[[Cortez et al., 2009]](http://www3.dsi.uminho.pt/pcortez/wine/).

Folder `step5` contains a copy of `step4`. First, we will have to
implement a Palladium `DatasetLoader` to read the dataset from
[OpenML.org](https://www.openml.org/).  We will then adapt the neural
network from our last example to do the classification.

Note that when you are finished implementing the DatasetLoader, you'll
see an error trying to fit the neural network because of a dimension
mismatch.

---

# A DatasetLoader for OpenML

The configuration in `step5/palladium-config.py` was updated to refer
to a new class `OpenML`, which will read the dataset from
[OpenML.org](https://www.openml.org):

```python
    'dataset_loader_train': {
        '__factory__': 'dataset.OpenML',
        'name': 'wine-quality-red',
    },
```

## ✍️ Exercise

Complete the implementation of the `OpenML` class in
`step5/dataset.py`.  The source file has important hints for you.

```python
from sklearn.datasets import fetch_openml

class OpenML:
    def __init__(self, name):
        self.name = name

    def __call__(self):
        # YOUR CODE HERE:
```

---

# MyModule: Input and output parameters

## ✍️ Exercise (continued)

The feature matrix of our new dataset has a shape (dimensionality)
that is different to the shape of our Iris feature matrix.  Also, the
number of classes is different.

Determine the shape of the new feature matrix by adding print
statements into your `OpenML` implementation.

Try to find out which parameters to set in `step5/palladium-config.py`
so that the network can be trained:

```python
    'model': {
        '__factory__': 'model.create_pipeline',
        # YOUR CODE HERE:
    },
```

As soon as you are able to train the network (use `pld-fit`), take a
look at the grid search parameters inside the configuration file.  Can
you find a configuration of hyperparameters with which you can reach
an accuracy of 55% or more on the `mean_test_score`?

---

# Regression instead of classification

We can pose the prediction of the wine quality score as a regression
problem.  That is, instead of predicting probabilities for the 10 or
so classes, we will try to predict a single number: the actual score
as a continuous value between 0 and 10.

## ✍️ Exercise

- We make a copy of `step5` and call the new directory `step6`
- Our network needs to have one output instead of ten
- The `softmax` function is not used in regression: remove it
- We use the `neg_mean_absolute_error` instead of `accuracy` in
  `palladium-config.py`
- We make use of `NeuralNetRegressor` instead of `NeuralNetClassifier`
- We'll replace our `PipelineY` with `sklearn.pipeline.Pipeline`.
  `LabelEncoder` and `predict_use_inverse` are no longer useful.
  (tricky!)
- Let's hack our `DatasetLoader` to return the target (or `y`) vector
  as `float32`: `target = dataset.target.astype('float32')`.
- For regression problems, skorch expects a target of shape NxM, where
  each sample can have multiple regression targets: `target =
  target.reshape(-1, 1)`.

---

# Random Forest instead of neural network

## ✍️ Exercise

- Rewrite the `model` entry in `step6/palladium-config.py` to use
  `sklearn.ensemble.RandomForestRegressor` instead of our
  `create_pipeline` function.

  *Hint:* You'll need the following lines in your configuration:

  ```python
      'model': {
          '__factory__': 'sklearn.ensemble.RandomForestRegressor',
          # ...
      },
  ```

- Read through the most important parameters of class
  [`RandomForestregressor` in the scikit-learn
  documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).
  Use a few parameters to run a grid search.  What's the best
  `mean_test_score` can you can get?

---

# Skorch: Sentiment analysis with RNNs

Our last skorch example demonstrates the use of RNNs to do sentiment
analysis on text.  We use the [Large Movie Review
Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) to train our
network.

The implementation follows the `rnn_classifier` example within the
skorch source tree.  The implementation is already provided for you in
`step7/palladium-config.py` and `step7/model.py`.  Before we dive into
how the model works, let's train it:

```bash
$ cd ../step7
$ pld-fit --evaluate
```

This may take a while if you're doing this on a CPU.

---

# Sentiment analysis as a service

After the training finished, we can fire up our web service again:

```bash
$ pld-devserver
```

And then call it like so:

http://localhost:5000/predict?text=A+fantastic+movie+despite+its+flaws

The response should look similar to this:

```json
{"result":[0.348106294870377,0.651893734931946],
 "metadata":{"status":"OK","error_code":0}}
```

---

# Skorch: Sentiment analysis with RNNs (2)

Let's take a look at the ["Predict sentiment on the IMDB Dataset"
notebook](https://github.com/dnouri/skorch/blob/master/examples/rnn_classifer/RNN_sentiment_classification.ipynb)
to understand how the model works.

```bash
$ pip install jupyter
$ jupyter notebook
```

Look for `RNN_sentiment_classification.ipynb`.

---

# Skorch: Debug network layers with pdb

To get a better intuition of what the individual layers of our neural
net actually do, we use the [Python
Debugger](https://docs.python.org/3/library/pdb.html) to step through
the `RNNClassifier.forward` method in `step7/model.py`.  Place a line
right after `def forward(self, X)`:

```python
    def forward(self, X):
        import pdb; pdb.set_trace()  # this is the new line!
        embeddings = self.emb(X)
        # from the recurrent layer, only take the activities from the
        # ...
```

And then run `pld-fit` again.  This will drop you into the debugger's
prompt.  Use the `help` command to find out what [debugger
commands](https://docs.python.org/3/library/pdb.html#debugger-commands)
exist:

```
(Pdb) help

Documented commands (type help <topic>):
========================================
EOF    c          d        h         list      q        rv       undisplay
a      cl         debug    help      ll        quit     s        unt
alias  clear      disable  ignore    longlist  r        source   until
args   commands   display  interact  n         restart  step     up
b      condition  down     j         next      return   tbreak   w
break  cont       enable   jump      p         retval   u        whatis
bt     continue   exit     l         pp        run      unalias  where
```

---

# Deploying models to production

We've seen how `pld-devserver` works for running a web service
locally, but how do we go about making available our model on an
actual server?

We will now walk through these steps:

- Set up an AWS account (if you don't have one yet)
- Set up an EC2 micro instance
- SSH into the instance to install dependencies and run the Palladium
  web service

---

# Register and sign in to AWS console

Go to the [aws.amazon.com](https://aws.amazon.com/) and click *Sign In
to the Console*.  If you don't have an account yet, you can register
by clicking *Create a new AWS account*.  It will ask you to put your
credit card information and telephone number.  However, the micro
instance that we use will cost you nothing.  (Just make sure you
remove it later.  It's free for one year.)

---

# Launch a new EC2 instance

Under the *Services* drop-down, click *EC2*.  If you're not using EC2
yet, it will say *0 Running Instances*.  Click this link and then
click the blue *Launch Instance* button to set up a micro instance.

In the first step, choose *Ubuntu Server 18.04 LTS* as the *Amazon
Machine Image (AMI)*.  In the second step, choose the *t2.micro*
instance type.  You can then click the blue *Review and Launch*
button.

Lastly, when you now click *Launch*, it will display a pop-up with
which you can *Create a new key pair*.  The name doesn't matter, maybe
choose something like *DSR Key* and click *Download Key Pair*.  This
will download a file with the ending *.pem*.  Keep this file in a safe
place; you will need it to log in to your instance.

After launching you should be able to view your instance in the
overview.

![:scale 100%](./resources/ec2-instances.png)

---

# Open up additional ports to your EC2 instance

In the overview of instances, look at the bottom pane and navigate to
*Security groups* and click *launch-wizard-1*.  You will now find
yourself on a screen that lists your security groups.  On the bottom
third of the screen, you'll see a tab called *Inbound*.  Click *Edit*
and then *Add Rule*.  From the *Type* dropdown, choose *HTTP*.  Then,
add another rule and choose *HTTPS*.  You can leave the defaults as is
and confirm by clicking the blue *Save* button.  We have now opened up
ports 80 and 443 on our EC2 instance.  This will be useful later on,
when we access our web service through these ports.

![:scale 80%](./resources/ec2-inbound-rules.png)

---

# SSH into your EC2 instance

Your EC2 instance has a public IPv4 address.  In this example it's
35.157.232.182.  Make sure you use your own IP address and not mine in
the following examples:

![:scale 100%](./resources/ec2-instances.png)

You may already have an SSH client installed.  Try:

```bash
$ ssh -V
OpenSSH_7.6p1 Ubuntu-4ubuntu0.2, OpenSSL 1.0.2n  7 Dec 2017
```

If you do, you can now log in to your EC2 instance by issuing this
command:

```bash
$ ssh -i path/to/DSRKey.pem ubuntu@35.157.232.182
```

Once you have successfully logged in, you should see a prompt like
this one:

```bash
ubuntu@ip-172-31-42-9:~$
```

---

# SSH into your EC2 instance: possible issues

If you're on Ubuntu and you don't have `ssh` installed, you can
install it like so:

```bash
$ sudo apt install openssh-client
```

If you encounter an error saying: *It is required that your private
key files are NOT accessible by others*, then run this command (once)
before you run `ssh`:

```bash
$ chmod 600 path/to/DSRKey.pem
$ ssh -i path/to/DSRKey.pem ubuntu@35.157.232.182
```

On newer versions of Windows, the OpenSSH client should be installed
by default in `C:\Windows\System32\OpenSSH`.  If it's not installed,
try navigating to *Settings App > Apps > Settings & Apps > Manage
Optional Features > Add Feature* and select the *OpenSSH Client*.

Once you have successfully logged in, you should see a prompt like
this one:

```bash
ubuntu@ip-172-31-42-9:~$
```

---

# Install dependencies on our EC2 instance

First, let's install a couple of dependencies that we'll need:

```bash
$ sudo apt update && sudo apt upgrade
$ sudo apt install python3-venv python3-dev build-essential
```

Next, we'll check out this repo from Github and create a virtualenv
inside of it:

```bash
$ git clone https://github.com/markus-hinsche/tut-productive-ml.git
...
$ python3 -m venv tut-productive-ml
$ cd tut-productive-ml
$ source bin/activate
```

The last command will change our prompt to look something like this:

```
(tut-productive-ml) ubuntu@ip-172-31-42-9:~/tut-productive-ml$
```

We can now proceed to install Python dependencies:


```bash
$ pip install -U pip
$ pip install -r step1/requirements.txt
...
Successfully installed ...
```

---

# Use Gunicorn to serve the web service

We've previously used `pld-devserver` to run requests against
Palladium's built-in web service.  `pld-devserver` uses Flask's
built-in web server that's suitable for development, but not for
production environments.  Instead, we'll now install and use
[Gunicorn](https://gunicorn.org/) to serve the web service:

```bash
$ pip install gunicorn
$ cd step3
$ pld-fit
$ gunicorn palladium.wsgi:app -b 0.0.0.0:8080
```

Try to navigate your browser to this URL, and you'll notice that it's
not yet working as expected.  There's no response:
http://35.157.232.182/predict?sepal%20length=6.3&sepal%20width=2.5&petal%20length=4.9&petal%20width=1.5

The problem is that we've asked Gunicorn to serve on port 8080, but
the standard HTTP port which our browser connects to is port 80.  The
issue is that only the `root` user can bind to port 80, and we don't
want to run our application as root.  What can we do?  We can use
[iptables](https://www.netfilter.org/):

```bash
$ sudo iptables -A PREROUTING -t nat -p tcp --dport 80 -j REDIRECT --to-ports 8080
```

---

# But is it fast? Use `ab` to find out!

A classic tool for benchmarking websites is `ab`, the Apache HTTP
server benchmarking tool.  Let's install and run a thousand requests
against our web service, with 10 requests at a time:

```bash
$ sudo apt install apache2-utils
$ ab -n 1000 -c 10 "http://localhost:8080/predict?sepal%20length=6.3&sepal%20width=2.5&petal%20length=4.9&petal%20width=1.5"
```

In the output, look for something like this:

```
Requests per second:    250.49 [#/sec] (mean)
```

We can serve 250 requests per second.  That's not bad at all.  But
let's have a closer look at where time is being spent.

---

# Profiling our web app

To see where time is being spent on each request, let's install the
[py-spy Python profiler](https://github.com/benfred/py-spy):

```bash
$ cd tut-productive-ml
$ source bin/activate
$ pip install py-spy
```

We keep the Gunicorn process running in another window.  Note that
Gunicorn prints out the process id (PID) of its worker when it starts
up.  It looks something like this:

```
[2020-11-18 16:12:05 +0000] [5883] [INFO] Booting worker with pid: 5883
```

We can now use `py-spy` to attach to the Gunicorn worker and see what
it's doing:

```bash
$ sudo su
$ source bin/activate
$ py-spy top --pid 5883  # replace with the actual Gunicorn PID you just saw
```

---

# Profiling our web app (2)

You should see output similar to `top`.  The web server isn't doing
much right now, but we can send another round of requests to it to see
what it's doing when it's busy.  In a new terminal window, run another
10000 requests using `ab` and switch back to the `py-spy` window to
see what's happening.

```bash
$ ab -n 10000 -c 10 "http://localhost:8080/predict?sepal%20length=6.3&sepal%20width=2.5&petal%20length=4.9&petal%20width=1.5"
```

![:scale 100%](./resources/py-spy.png)

How come we're spending most of our time executing SQL statements?
What's the `read` function in `palladium.persistence` at line 421
doing?  Find out!

---

# Profiling our web app (3)

The culprit is the Palladium `model_persister`, which is set up to
read the persisted model from disk on every request!  This is not
useful at all.  Thankfully, there's a wrapper called
`CachedUpdatePersister` that fixes this issue, as it keeps the model
in memory.  Replace the `model_perister` entry in the
`step3/palladium-config.py` configuration file with the following:

```python
    'model_persister': {
        '__factory__': 'palladium.persistence.CachedUpdatePersister',
        'impl': {
            '__factory__': 'palladium.persistence.Database',
            'url': 'sqlite:///iris-model.db',
        },
    },
```

Now run the `ab` test again.  How many requests per second do you get?

---

# What we did so far with our EC2 instance:

- Set up the EC2 instance using the AWS console

- Opened up additional network ports

- Logged in using SSH and cloned this repo from Github

- Installed system dependencies (`apt install`) and Python project
  dependencies (`pip install`) on our EC2 instance

- Used Gunicorn to serve the Palladium web service

- Benchmarked (`ab`) and profiled (`py-spy`) our web service

---

# What we'll need to do next:

- We're running the `gunicorn` process in our terminal in foreground.
  If we close the terminal window, Gunicorn will stop working.  To fix
  this issue, we'll use [Supervisor](http://supervisord.org/) to start
  up the Gunicorn process and keep it running.

- Anyone who knows our web service's address can now use it.  What's
  worse, we're sending the data to predict for and the results over
  the wire, without encryption.  We'll use HTTPS instead of HTTP, and
  use [basic access
  authentication](https://en.wikipedia.org/wiki/Basic_access_authentication)
  to prevent eavesdropping and unauthorized use.

---

# Configure Supervisor to keep Gunicorn running

Use `Ctrl+C` to shut down the `gunicorn` process (or alternatively,
close the terminal window in which you're running it).

On the server, create this supervisor config file and save it as
`/home/ubuntu/tut-productive-ml/supervisor.conf`:

```ini
[program:webservice]
command=/home/ubuntu/tut-productive-ml/bin/gunicorn palladium.wsgi:app -b 0.0.0.0:8080
directory=/home/ubuntu/tut-productive-ml/step3/
user=ubuntu
environment=PALLADIUM_CONFIG="palladium-config.py"
```

You can find out more about [Supervisor's configuration files in the
documentation](http://supervisord.org/configuration.html).

---

# Install and run Supervisor

We install Supervisor with `apt install`:

```bash
$ sudo apt install supervisor
```

And we link the configuration file that we just wrote into a directory
where Supervisor will pick it up:

```bash
$ sudo ln -s /home/ubuntu/tut-productive-ml/supervisor.conf /etc/supervisor/conf.d/
```

Now let's restart Supervisor and check if it's running our process:

```bash
$ sudo /etc/init.d/supervisor restart
$ sudo supervisorctl status
webservice                       RUNNING   pid 6630, uptime 0:00:22
```

At this point, we can log out from the server and the web service will
keep running!  What's more, Supervisor will start up our web service
when the EC2 box is ever restarted, and it will restart our web
service should it ever crash with a Python exception or the like.

---

# End

- https://github.com/ottogroup/palladium
- https://github.com/scikit-learn/scikit-learn
- https://github.com/skorch-dev/skorch
- https://github.com/dask/dask


- mail@markushinsche.de
- [linkedin.com/in/markushinsche](https://linkedin.com/in/markushinsche)
- [twitter.com/markus_hinsche](https://twitter.com/markus_hinsche)
- [markushinsche.de](https://markushinsche.de)

Thanks to Daniel Nouri for initial versions of the slide deck

## Images

- [scikit-learn](http://scikit-learn.org)
- [Machine Learning
  course](https://www.coursera.org/learn/machine-learning) on Coursera

    </textarea>
    <script src="resources/remark.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      remark.macros.scale = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
      var slideshow = remark.create();
    </script>
  </body>
</html>
